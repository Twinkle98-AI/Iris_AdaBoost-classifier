ğŸŒ¸ Iris Flower Classification using AdaBoost
ğŸ“Œ Project Overview

This project demonstrates the implementation of the AdaBoost (Adaptive Boosting) algorithm to classify Iris flower species based on their physical characteristics. AdaBoost is a powerful ensemble learning technique that combines multiple weak learners to form a strong classifier and improve prediction accuracy.

The project also includes a Streamlit-based interactive frontend, allowing users to input flower measurements and get real-time predictions.

ğŸ§  Problem Statement

Given the measurements of an Iris flower:

Sepal Length

Sepal Width

Petal Length

Petal Width

Predict the species of the flower:

Setosa

Versicolor

Virginica

ğŸ—‚ Dataset

Source: Iris Dataset (UCI / Kaggle)

Samples: 150

Features: 4 numerical features

Target: Species (3 classes)

âš™ï¸ Tech Stack

Python

Scikit-learn

AdaBoost Classifier

Decision Tree (Weak Learner)

NumPy & Pandas

Streamlit

Pickle (Model Serialization)

ğŸ— Model Architecture

Ensemble Method: Boosting (Sequential)

Base Estimator: Decision Tree (max_depth = 1)

Number of Estimators: 50

Learning Rate: 1.0

ğŸš€ Features

End-to-end ML pipeline

Label encoding for categorical target

Trainâ€“test split for evaluation

Streamlit-based user interface

Real-time prediction

Production-ready serialized model (.pkl)

ğŸ“Š Model Performance

Accuracy: ~86â€“91% (depending on base estimator)

Demonstrates improved performance through ensemble learning

ğŸ–¥ Frontend Preview

The Streamlit app allows users to:

Enter flower measurements

Click Predict

Instantly view predicted Iris species

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Clone the repository
git clone https://github.com/your-username/iris-adaboost-classifier.git
cd iris-adaboost-classifier

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run the Streamlit app
streamlit run iris.py

ğŸ“ˆ Key Learnings

Understanding ensemble learning techniques

Difference between Bagging and Boosting

Practical implementation of AdaBoost

Importance of weak learners

Model deployment using Streamlit

ğŸ“Œ Future Enhancements

Probability visualization

Model comparison (AdaBoost vs GBM vs XGBoost)

Hyperparameter tuning

Cloud deployment

ğŸ¤ Acknowledgements

Thanks to the open-source community and mentors for guidance and continuous learning support.

ğŸ“¬ Contact

Haimabati HaripriyaSahu
Aspiring Data Scientist | Machine Learning Enthusiast
